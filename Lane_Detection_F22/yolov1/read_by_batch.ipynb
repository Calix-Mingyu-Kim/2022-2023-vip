{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import YOLOv1\n",
    "from loss import YoloLoss\n",
    "from bdd100k import BDD100k\n",
    "from utils import xywh_to_xyxy, xyxy_to_xywh, intersection_over_union, non_max_supression, precision, recall\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((448,448))])\n",
    "val_dataset = BDD100k(root='/Users/calixkim/VIP27920/bdd100k/', train=False, transform=transform)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, \n",
    "                            batch_size=8,\n",
    "                            num_workers=2,\n",
    "                            shuffle=False)\n",
    "\n",
    "model_ = YOLOv1(split_grids=7, num_bboxes=2, num_classes=13)\n",
    "model_.load_state_dict(torch.load('/Users/calixkim/VIP27920/Lane_Detection_F22/yolov1//weights/yolov1_150.pt', map_location=torch.device('cpu')))\n",
    "model_.eval();\n",
    "\n",
    "def unnorm_bbox(bbox, width, height, i, j):\n",
    "    '''\n",
    "    Unormalize the bounding box predictions based on the yolo predictions\n",
    "    '''\n",
    "    bbox[:,0] = width / (7 / (bbox[:,0] + j))\n",
    "    bbox[:,1] = height / (7 / (bbox[:,1] + i))\n",
    "    bbox[:,2] = width / (7 / bbox[:,2])\n",
    "    bbox[:,3] = height / (7 / bbox[:,3])\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def draw_data_loader(imgs, labels):\n",
    "    '''\n",
    "    Draw the predicted bounding boxes from the network on a single image\n",
    "    Args:\n",
    "        imgs (tensor): images of the predicted bboxes\n",
    "        labels (tensor): ground truth bboxes\n",
    "    Returns:\n",
    "        ground_truth (numpy array): ground truth bboxes with object in it. [batch #, x, y, w, h]\n",
    "    '''\n",
    "    b, _, height, width = imgs.shape\n",
    "    imgs = imgs.to(torch.uint8)\n",
    "    \n",
    "    S = labels.shape[1]\n",
    "    conf = labels[..., 13]\n",
    "    I = torch.nonzero(conf).numpy()\n",
    "    \n",
    "    batch, i, j = I[:,0], I[:,1], I[:,2] \n",
    "    bbox = labels[batch,i,j,14:18]\n",
    "    bbox[...,0] = width / (S / (bbox[...,0] + j))\n",
    "    bbox[...,1] = height / (S / (bbox[...,1] + i))\n",
    "    bbox[...,2] = width / (S / bbox[...,2])\n",
    "    bbox[...,3] = height / (S / bbox[...,3])\n",
    "    bbox = xywh_to_xyxy(bbox, width, height)\n",
    "    batch = np.expand_dims(batch, axis=0)\n",
    "\n",
    "    ground_truth = np.concatenate((batch.T,bbox),axis=1)\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def draw_batch(img, pred, out_dir='./', display=False):\n",
    "    '''\n",
    "    Draw the predicted bounding boxes from the network on a single image\n",
    "    Args:\n",
    "        img (tensor): image of the predicted bboxes\n",
    "        pred (tensor): predicted bboxes\n",
    "        out_dir (str): output directory to store image\n",
    "        display (bool): display the image or not\n",
    "    Returns:\n",
    "    '''\n",
    "    class_dict = {\n",
    "        'pedestrian' : 1,\n",
    "        'rider' : 2,\n",
    "        'car' : 3,\n",
    "        'truck' : 4, \n",
    "        'bus' : 5, \n",
    "        'train' : 6, \n",
    "        'motorcycle' : 7,\n",
    "        'bicycle' : 8,\n",
    "        'traffic light' : 9,\n",
    "        'traffic sign' : 10,\n",
    "        'other vehicle': 11,\n",
    "        'trailer': 12,\n",
    "        'other person': 13,\n",
    "    }\n",
    "    num_to_class = {i:s for s,i in class_dict.items()}\n",
    "\n",
    "    s = pred.shape[-1]\n",
    "    img = torch.tensor(img) if not isinstance(img, torch.Tensor) else img\n",
    "    assert type(pred) == torch.Tensor\n",
    "    img = img.to(torch.uint8)\n",
    "    batch, _, height, width = img.shape\n",
    "    nms = []\n",
    "    _, pclass = torch.max(pred[:, :, :13], dim=2)\n",
    "    for i in range(s):\n",
    "        for j in range(s):\n",
    "            conf = pred[i, j, 13]\n",
    "            if conf > 0.5:\n",
    "                class_pred = num_to_class[pclass[i, j].item()]\n",
    "                label = [class_pred]\n",
    "                bbox1 = pred[i, j, 14:18].unsqueeze(0)\n",
    "                bbox1 = unnorm_bbox(bbox1, width, height, i ,j)\n",
    "                bbox1 = xywh_to_xyxy(bbox1, width, height)\n",
    "                bbox2 = pred[i, j, 19:23].unsqueeze(0)\n",
    "                bbox2 = unnorm_bbox(bbox2, width, height, i ,j)\n",
    "                bbox2 = xywh_to_xyxy(bbox2, width, height)\n",
    "                img = draw_bounding_boxes(img, bbox1, width=3, labels=label, colors=(0, 255, 0)) \n",
    "                img = draw_bounding_boxes(img, bbox2, width=3, labels=label, colors=(0, 255, 0))\n",
    "                class_conf = torch.stack((torch.tensor((class_dict[class_pred])), conf))\n",
    "                nms.append(torch.cat((class_conf, pred[i, j, 14:18])))\n",
    "                nms.append(torch.cat((class_conf, pred[i, j, 19:23])))\n",
    "\n",
    "    # nms = sorted(nms, key=lambda x: x[1])\n",
    "    nms = non_max_supression(nms, 0.5, 0.5)\n",
    "    img = np.transpose(img.cpu().numpy(), (1,2,0)).astype(np.uint8)\n",
    "    \n",
    "    return img, nms\n",
    "    \n",
    "\n",
    "def draw_nms(img, nms, out_dir='./', display=False):\n",
    "    '''\n",
    "    Draw the predicted bounding boxes from the network on a single image\n",
    "    Args:\n",
    "        img (tensor): image of the predicted bboxes\n",
    "        pred (tensor): predicted bboxes\n",
    "        out_dir (str): output directory to store image\n",
    "        display (bool): display the image or not\n",
    "    Returns:\n",
    "    '''\n",
    "    class_dict = {\n",
    "        'pedestrian' : 1,\n",
    "        'rider' : 2,\n",
    "        'car' : 3,\n",
    "        'truck' : 4, \n",
    "        'bus' : 5, \n",
    "        'train' : 6, \n",
    "        'motorcycle' : 7,\n",
    "        'bicycle' : 8,\n",
    "        'traffic light' : 9,\n",
    "        'traffic sign' : 10,\n",
    "        'other vehicle': 11,\n",
    "        'trailer': 12,\n",
    "        'other person': 13,\n",
    "    }\n",
    "    num_to_class = {i:s for s,i in class_dict.items()}\n",
    "\n",
    "    img = torch.tensor(img) if not isinstance(img, torch.Tensor) else img\n",
    "    img = img.to(torch.uint8)\n",
    "    batch, _, height, width = img.shape\n",
    "\n",
    "    for x in nms:\n",
    "        bbox = xywh_to_xyxy(x[2:].unsqueeze(0), width, height)\n",
    "        label = str(round((x[1].item()),2))\n",
    "        img = draw_bounding_boxes(img, bbox, width=3, labels=[label], colors=(0,255,0))\n",
    "\n",
    "    img = np.transpose(img.cpu().numpy(), (1,2,0)).astype(np.uint8)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Recall\n",
    "def accuracy(label, nms):\n",
    "    '''\n",
    "    Args:\n",
    "        label (tensor): ground truth bboxes\n",
    "        nms (list): list of tensors after non max suppression. [class, conf, x, y, w, h]\n",
    "    Returns:\n",
    "        tp (int): True Positive\n",
    "        fp (int): False Positive\n",
    "        fn (int): False Negative\n",
    "        tn (int): True Negative\n",
    "    '''\n",
    "\n",
    "    tp, fp, fn, tn = (0, 0, 0, 0)\n",
    "    nms = torch.stack((nms))\n",
    "\n",
    "    ground_truth_all = []   # [0|1, x, y, w, h] * 49\n",
    "    s = label.shape[1]\n",
    "    for i in range(s):\n",
    "        for j in range(s):\n",
    "            ground_truth_all.append(label[i, j, 13:18])\n",
    "\n",
    "    for pred in nms:\n",
    "        iou_max = 0\n",
    "        for i, v in enumerate(ground_truth_all):\n",
    "            iou = intersection_over_union(v[1:].unsqueeze(0), pred[2:].unsqueeze(0))\n",
    "            if iou > iou_max:\n",
    "                iou_max = iou\n",
    "                gt_max_idx = i\n",
    "\n",
    "        if iou_max > 0.1:   # detected\n",
    "            print(ground_truth_all[gt_max_idx])\n",
    "            if ground_truth_all[gt_max_idx][0] == 0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "    tn = 49 - tp - fp - fn\n",
    "    \n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change, copy paste to actual function later\n",
    "\n",
    "def draw_data_loader(imgs, labels):\n",
    "    '''\n",
    "    Draw the predicted bounding boxes from the network on a single image\n",
    "    Args:\n",
    "        imgs (tensor): images of the predicted bboxes\n",
    "        labels (tensor): ground truth bboxes\n",
    "    Returns:\n",
    "        ground_truth (numpy array): ground truth bboxes with object in it. [batch #, x, y, w, h]\n",
    "    '''\n",
    "    b, _, height, width = imgs.shape\n",
    "    imgs = imgs.to(torch.uint8)\n",
    "    \n",
    "    S = labels.shape[1]\n",
    "    conf = labels[..., 13]\n",
    "    I = torch.nonzero(conf).numpy()\n",
    "    \n",
    "    batch, i, j = I[:,0], I[:,1], I[:,2] \n",
    "    bbox = labels[batch,i,j,14:18]\n",
    "    bbox[...,0] = width / (S / (bbox[...,0] + j))\n",
    "    bbox[...,1] = height / (S / (bbox[...,1] + i))\n",
    "    bbox[...,2] = width / (S / bbox[...,2])\n",
    "    bbox[...,3] = height / (S / bbox[...,3])\n",
    "    bbox = xywh_to_xyxy(bbox, width, height)\n",
    "    batch = np.expand_dims(batch, axis=0)\n",
    "\n",
    "    ground_truth = np.concatenate((batch.T,bbox),axis=1)\n",
    "    \n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN, TN = 0, 0, 0, 0\n",
    "rec = []\n",
    "prec = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: [[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]]\n",
      "i  [1 2 3]\n",
      "[1 2 3] [1 2 3]\n",
      "torch.Size([8, 3, 4])\n",
      "tensor([[0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.3000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000],\n",
      "        [0.3000, 0.7000, 0.1000]])\n",
      "tensor([[[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000, 102.4000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 185.6000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  19.2000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]],\n",
      "\n",
      "        [[ 70.4000,  76.8000,  19.2000,  25.6000],\n",
      "         [160.0000, 166.4000,  44.8000,  51.2000],\n",
      "         [198.4000, 204.8000,   6.4000,  12.8000]]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pb/2zs7s0h52cldpmyx2v1l6w880000gn/T/ipykernel_28775/3689282409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxywh_to_xyxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#     img = draw_bounding_boxes(img, bbox, width=3, colors=(0,255,0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VIP27920/Lane_Detection_F22/yolov1/utils.py\u001b[0m in \u001b[0;36mxywh_to_xyxy\u001b[0;34m(bbox, width, height)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mbbox_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mbbox_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mbbox_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mbbox_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mbbox_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 1 with size 3"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "test = torch.rand(8,7,7,18)\n",
    "img_ = torch.rand(8, 3, 448, 448)\n",
    "test[1,1,1,13] = 1\n",
    "test[2,2,2,13] = 1\n",
    "test[3,3,3,13] = 1\n",
    "test = test.to(torch.uint8)\n",
    "batch, _, height, width = img_.shape\n",
    "# print(test[..., 13:18].shape) # 8 images, [conf, x, y, w, h] \n",
    "conf = test[..., 13]\n",
    "I = torch.nonzero(conf).numpy()\n",
    "batch = I[:,0]\n",
    "i = I[:,1]\n",
    "j = I[:,2]\n",
    "# print(i,j)\n",
    "bbox = test[:,i,j,14:18]\n",
    "print(bbox.shape) # -> torch.Size([N, 4]) N: number of nonzero conf in ground truth\n",
    "bbox = torch.tensor([[[0.1,0.2,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.6,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.9,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.6,0.3,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]],\n",
    "                    [[0.1,0.2,0.3,0.4], [0.5,0.6,0.7,0.8], [0.1, 0.2, 0.1, 0.2]]])\n",
    "print(bbox[..., 2])\n",
    "bbox[...,0] = width / (7 / (bbox[...,0] + j))\n",
    "bbox[...,1] = height / (7 / (bbox[...,1] + i))\n",
    "bbox[...,2] = width / (7 / bbox[...,2])\n",
    "bbox[...,3] = height / (7 / bbox[...,3])\n",
    "print(bbox)\n",
    "bbox = xywh_to_xyxy(bbox, width, height)\n",
    "print(bbox)\n",
    "    #     img = draw_bounding_boxes(img, bbox, width=3, colors=(0,255,0))\n",
    "\n",
    "    # img = np.transpose(img.cpu().numpy(), (1,2,0)).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "(1, 62) (1, 62)\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [29]\n",
      " [30]\n",
      " [31]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [48]\n",
      " [49]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]\n",
      " [54]\n",
      " [55]\n",
      " [56]\n",
      " [57]\n",
      " [58]\n",
      " [59]\n",
      " [60]\n",
      " [61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 2.,  1.,  1.,  1.,  1.],\n",
       "       [ 3.,  1.,  1.,  1.,  1.],\n",
       "       [ 4.,  1.,  1.,  1.,  1.],\n",
       "       [ 5.,  1.,  1.,  1.,  1.],\n",
       "       [ 6.,  1.,  1.,  1.,  1.],\n",
       "       [ 7.,  1.,  1.,  1.,  1.],\n",
       "       [ 8.,  1.,  1.,  1.,  1.],\n",
       "       [ 9.,  1.,  1.,  1.,  1.],\n",
       "       [10.,  1.,  1.,  1.,  1.],\n",
       "       [11.,  1.,  1.,  1.,  1.],\n",
       "       [12.,  1.,  1.,  1.,  1.],\n",
       "       [13.,  1.,  1.,  1.,  1.],\n",
       "       [14.,  1.,  1.,  1.,  1.],\n",
       "       [15.,  1.,  1.,  1.,  1.],\n",
       "       [16.,  1.,  1.,  1.,  1.],\n",
       "       [17.,  1.,  1.,  1.,  1.],\n",
       "       [18.,  1.,  1.,  1.,  1.],\n",
       "       [19.,  1.,  1.,  1.,  1.],\n",
       "       [20.,  1.,  1.,  1.,  1.],\n",
       "       [21.,  1.,  1.,  1.,  1.],\n",
       "       [22.,  1.,  1.,  1.,  1.],\n",
       "       [23.,  1.,  1.,  1.,  1.],\n",
       "       [24.,  1.,  1.,  1.,  1.],\n",
       "       [25.,  1.,  1.,  1.,  1.],\n",
       "       [26.,  1.,  1.,  1.,  1.],\n",
       "       [27.,  1.,  1.,  1.,  1.],\n",
       "       [28.,  1.,  1.,  1.,  1.],\n",
       "       [29.,  1.,  1.,  1.,  1.],\n",
       "       [30.,  1.,  1.,  1.,  1.],\n",
       "       [31.,  1.,  1.,  1.,  1.],\n",
       "       [32.,  1.,  1.,  1.,  1.],\n",
       "       [33.,  1.,  1.,  1.,  1.],\n",
       "       [34.,  1.,  1.,  1.,  1.],\n",
       "       [35.,  1.,  1.,  1.,  1.],\n",
       "       [36.,  1.,  1.,  1.,  1.],\n",
       "       [37.,  1.,  1.,  1.,  1.],\n",
       "       [38.,  1.,  1.,  1.,  1.],\n",
       "       [39.,  1.,  1.,  1.,  1.],\n",
       "       [40.,  1.,  1.,  1.,  1.],\n",
       "       [41.,  1.,  1.,  1.,  1.],\n",
       "       [42.,  1.,  1.,  1.,  1.],\n",
       "       [43.,  1.,  1.,  1.,  1.],\n",
       "       [44.,  1.,  1.,  1.,  1.],\n",
       "       [45.,  1.,  1.,  1.,  1.],\n",
       "       [46.,  1.,  1.,  1.,  1.],\n",
       "       [47.,  1.,  1.,  1.,  1.],\n",
       "       [48.,  1.,  1.,  1.,  1.],\n",
       "       [49.,  1.,  1.,  1.,  1.],\n",
       "       [50.,  1.,  1.,  1.,  1.],\n",
       "       [51.,  1.,  1.,  1.,  1.],\n",
       "       [52.,  1.,  1.,  1.,  1.],\n",
       "       [53.,  1.,  1.,  1.,  1.],\n",
       "       [54.,  1.,  1.,  1.,  1.],\n",
       "       [55.,  1.,  1.,  1.,  1.],\n",
       "       [56.,  1.,  1.,  1.,  1.],\n",
       "       [57.,  1.,  1.,  1.,  1.],\n",
       "       [58.,  1.,  1.,  1.,  1.],\n",
       "       [59.,  1.,  1.,  1.,  1.],\n",
       "       [60.,  1.,  1.,  1.,  1.],\n",
       "       [61.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((1,62))\n",
    "c = np.arange(0,62)\n",
    "print(a)\n",
    "c = np.expand_dims(c, axis=0)\n",
    "print(a.shape, c.shape)\n",
    "print(c.T)\n",
    "b = np.ones((62,4))\n",
    "np.concatenate((c.T,b),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f973462e4d3620acb3fc6f132437a932600befe43ded68c1f13d9935c627d15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
